---
title: 使用Python学习SVM分类
layout: post
tags:
  - SVM
  - Linear SVC
  - Python
---
如图找一条直线把两种颜色的点分开，最适用的算法是Linear SVC。在将算法应用到我们的数据上前，先稍微介绍一下Linear SVC是如何使用的，好让大家明白我们接下步骤是如何进行的。

Linear SVC (Support Vector Classifier)的作用就是根据你输入的数据，给出一个“最合适”的超平面，对你的数据进行分类。得到分类超平面之后，你就可以喂给你的分类器一些数据，分类器会根据这个超平面返回分类结果。好了，现在开始（假设你了解基本的Python语言）。


**首先设置好基本依赖条件**


	import numpy as np
	import matplotlib.pyplot as plt
	from matplotlib import style
	style.use("ggplot")
	from sklearn import svm

Matplotlib不是必须的，这里用它来可视化数据。一般来说，你不可能可视化任意维度的数据，这里我们的问题只有二维数据，可视化可以帮我们理解Linear SVC是如何运作的。

除了画图包我们还用到了sklearn里的svm包和numpy中的array包。

**接下来，假设我们有两组特征（数据）要考虑。这些特征待会我们当成二维空间的坐标处理。特征数据类似下面**

	x = [1, 5, 1.5, 8, 1, 9]
	y = [2, 8, 1.8, 8, 0.6, 11]

画图语句

	plt.scatter(x,y)
	plt.show()

结果如下：

![figure_1.png](/media/files/2016/04/21/figure_1.png)

好的，我们自己一眼就能看出来这些点能分成两类，尽管线条的具体位置还有待商榷：

![figure_2.png](/media/files/2016/04/21/figure_2.png)

两个特征可以画2D图像，三个特征就能用3D图像表示，3D的图在视觉上分类会困难一些，不过还是可以的。当特征达到四个往上甚至四千个就不可能从视觉上去区分了。但是机器学习能够看到并且分析人类无法想象的维度。

记住这点，我们接着解决问题。为了把数据喂给分类器，我们先要将数据放进规范化的数组，而不是光秃秃的x和y的坐标值。

**一般地，我们将特征列表存储在大写的X变量中。将上面的x和y的坐标值放进数组，其中x和y都是特征。**
	
	X = np.array([[1,2],
             [5,8],
             [1.5,1.8],
             [8,8],
             [1,0.6],
             [9,11]])

有了特征数组，给这些数据加上标记label来训练分类器。这种分类器叫做“监督式学习”，相应的不使用特征标记label的机器学习就是“非监督式学习”。

**标记label，有时候叫做“目标”，这里我们使用0或者1**

	y = [0,1,0,1,0,1]

回到我们的数据集，图中可以清晰地看到处于左下角比较“低”的部分的数据和右上角比教“高”的数据，姑且将“低”数据打上“0”标记，将“高”数据打上“1”标记。

**接下来定义分类器：**

	clf = svm.SVC(kernel='linear', C = 1.0)

使用支持向量机SVM中的支持向量类SVC，线性核函数，惩罚因子C取1.0。如果你不知道C的作用也不用着急，姑且看成是对分类器表现的调节参数。机器学习领域还很新，关于C的处理还存在许多争议，这里就取默认的1.0。

**训练分类器**

	clf.fit(X,y)

到这里，分类器的学习过程就结束了，由于数据量很少，训练应该是瞬间的事情。

**预测**

	print(clf.predict([0.58,0.76]))

我们祈祷分类器的预测结果是“0”，因为我们给的是一个“低”数据。好棒！分类结果的确是我们想要的。别着急，再输入：

	print(clf.predict([10.58,10.76]))

分类器又正确地给出了答案！！这些“盲目的”预测仍然是有效的测试，因为我们知道正确的答案。恭喜，你得到了100%正确率的分类器！！！

**好了，把得到的超平面画出来看看：**

	w = clf.coef_[0]
	print(w)

	a = -w[0] / w[1]

	xx = np.linspace(0,12)
	yy = a * xx - clf.intercept_[0] / w[1]

	h0 = plt.plot(xx, yy, 'k-', label="non weighted div")

	plt.scatter(X[:, 0], X[:, 1], c = y)
	plt.legend()
	plt.show()

**结果：**

![figure_3.png](/media/files/2016/04/21/figure_3.png)

将数据以图像形式表现出来有助于理解程序的运行过程中都干了什么，在算法实现过程中不是必须的。你会发现实际应用中大部分的数据是不能用图形表示出来的，因为有太多的特征（维度）了。

